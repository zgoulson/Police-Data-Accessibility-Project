# What problem(s) are we trying to solve with this framework?
- Scrapy is good with a static website, but can't do dynamic things. (site state parsing is difficult)
- Can be built with data lineage support, possibly specific to PDAP.
- Data duplication will be a problem that we can potentially stop on the front lines.
- Strict(er) control over storage variability.

## What is the expected output or outputs?
- Stored entire HTTP response.
- JSON Manifest - written last, will allow us to store all relevant metadata according to files.

## What scraping "type" are we supporting first?
- Start with REST APIs

## How much is configurable? How is it configured?
- Base URL
- Expected JSON Schema
- Start URI? End Date/Start Date
- Retry Logic
- Rate Limiting
- API Key (Authorization for content retrieval.)
- Scraper Key (Authorization for content storage.)

## Performance Metrics
- Error Logging (to filesystem for now)

## Test Coverage Metrics
- Unit Tests in all places applicable.

### Additional considerations going forward:
- Source Refresh Rate: How frequently does content at this source change?
